{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1_Score</th>\n",
       "      <th>A2_Score</th>\n",
       "      <th>A3_Score</th>\n",
       "      <th>A4_Score</th>\n",
       "      <th>A5_Score</th>\n",
       "      <th>A6_Score</th>\n",
       "      <th>A7_Score</th>\n",
       "      <th>A8_Score</th>\n",
       "      <th>A9_Score</th>\n",
       "      <th>A10_Score</th>\n",
       "      <th>...</th>\n",
       "      <th>gender</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>jundice</th>\n",
       "      <th>austim</th>\n",
       "      <th>contry_of_res</th>\n",
       "      <th>used_app_before</th>\n",
       "      <th>result</th>\n",
       "      <th>age_desc</th>\n",
       "      <th>relation</th>\n",
       "      <th>Class/ASD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>White-European</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>'United States'</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>Self</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>m</td>\n",
       "      <td>Latino</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>Self</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>m</td>\n",
       "      <td>Latino</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>Spain</td>\n",
       "      <td>no</td>\n",
       "      <td>8</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>Parent</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>White-European</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>'United States'</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>?</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>?</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   A1_Score  A2_Score  A3_Score  A4_Score  A5_Score  A6_Score  A7_Score  \\\n",
       "0         1         1         1         1         0         0         1   \n",
       "1         1         1         0         1         0         0         0   \n",
       "2         1         1         0         1         1         0         1   \n",
       "3         1         1         0         1         0         0         1   \n",
       "4         1         0         0         0         0         0         0   \n",
       "\n",
       "   A8_Score  A9_Score  A10_Score    ...    gender       ethnicity jundice  \\\n",
       "0         1         0          0    ...         f  White-European      no   \n",
       "1         1         0          1    ...         m          Latino      no   \n",
       "2         1         1          1    ...         m          Latino     yes   \n",
       "3         1         0          1    ...         f  White-European      no   \n",
       "4         1         0          0    ...         f               ?      no   \n",
       "\n",
       "  austim    contry_of_res used_app_before result       age_desc relation  \\\n",
       "0     no  'United States'              no      6  '18 and more'     Self   \n",
       "1    yes           Brazil              no      5  '18 and more'     Self   \n",
       "2    yes            Spain              no      8  '18 and more'   Parent   \n",
       "3    yes  'United States'              no      6  '18 and more'     Self   \n",
       "4     no            Egypt              no      2  '18 and more'        ?   \n",
       "\n",
       "  Class/ASD  \n",
       "0       YES  \n",
       "1       YES  \n",
       "2       YES  \n",
       "3        NO  \n",
       "4       YES  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data1=pd.read_csv('C:/Users/Krishna/Desktop/Autism-Adult-Data Plus Description File/Autism-Adult-Data.csv')\n",
    "d1=pd.DataFrame(data1)\n",
    "data2=pd.read_csv('D:/A/ANN/project/Autism-Screening-Child-Data Plus Description/Autism-Child-Data.csv')\n",
    "d2=pd.DataFrame(data2)\n",
    "data=[d1, d2]\n",
    "data=pd.concat(data)\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1_Score</th>\n",
       "      <th>A2_Score</th>\n",
       "      <th>A3_Score</th>\n",
       "      <th>A4_Score</th>\n",
       "      <th>A5_Score</th>\n",
       "      <th>A6_Score</th>\n",
       "      <th>A7_Score</th>\n",
       "      <th>A8_Score</th>\n",
       "      <th>A9_Score</th>\n",
       "      <th>A10_Score</th>\n",
       "      <th>...</th>\n",
       "      <th>gender</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>jundice</th>\n",
       "      <th>austim</th>\n",
       "      <th>contry_of_res</th>\n",
       "      <th>used_app_before</th>\n",
       "      <th>result</th>\n",
       "      <th>age_desc</th>\n",
       "      <th>relation</th>\n",
       "      <th>Class/ASD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>White-European</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>'United States'</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>Self</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>m</td>\n",
       "      <td>Latino</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>Self</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>m</td>\n",
       "      <td>Latino</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>Spain</td>\n",
       "      <td>no</td>\n",
       "      <td>8</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>Parent</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>White-European</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>'United States'</td>\n",
       "      <td>no</td>\n",
       "      <td>6</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>Self</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>White-European</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>'18 and more'</td>\n",
       "      <td>Self</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   A1_Score  A2_Score  A3_Score  A4_Score  A5_Score  A6_Score  A7_Score  \\\n",
       "0         1         1         1         1         0         0         1   \n",
       "1         1         1         0         1         0         0         0   \n",
       "2         1         1         0         1         1         0         1   \n",
       "3         1         1         0         1         0         0         1   \n",
       "4         1         0         0         0         0         0         0   \n",
       "\n",
       "   A8_Score  A9_Score  A10_Score    ...    gender       ethnicity jundice  \\\n",
       "0         1         0          0    ...         f  White-European      no   \n",
       "1         1         0          1    ...         m          Latino      no   \n",
       "2         1         1          1    ...         m          Latino     yes   \n",
       "3         1         0          1    ...         f  White-European      no   \n",
       "4         1         0          0    ...         f  White-European      no   \n",
       "\n",
       "  austim    contry_of_res used_app_before result       age_desc relation  \\\n",
       "0     no  'United States'              no      6  '18 and more'     Self   \n",
       "1    yes           Brazil              no      5  '18 and more'     Self   \n",
       "2    yes            Spain              no      8  '18 and more'   Parent   \n",
       "3    yes  'United States'              no      6  '18 and more'     Self   \n",
       "4     no            Egypt              no      2  '18 and more'     Self   \n",
       "\n",
       "  Class/ASD  \n",
       "0       YES  \n",
       "1       YES  \n",
       "2       YES  \n",
       "3        NO  \n",
       "4       YES  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data.replace('?',np.nan)#replacing ? with Nan\n",
    "data=data.fillna(method='ffill')#forward-fill to propagate the previous value forward\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "NO     639\n",
       "YES    357\n",
       "Name: Class/ASD, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "class_count = data['Class/ASD'].value_counts()\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.barplot(class_count.index, class_count.values, alpha=0.9)\n",
    "plt.title('Distribution of ASD classes')\n",
    "plt.ylabel('Number of cases', fontsize=12)\n",
    "plt.xlabel('Classification', fontsize=12)\n",
    "plt.show()\n",
    "class_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1_Score</th>\n",
       "      <th>A2_Score</th>\n",
       "      <th>A3_Score</th>\n",
       "      <th>A4_Score</th>\n",
       "      <th>A5_Score</th>\n",
       "      <th>A6_Score</th>\n",
       "      <th>A7_Score</th>\n",
       "      <th>A8_Score</th>\n",
       "      <th>A9_Score</th>\n",
       "      <th>A10_Score</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>jaundice</th>\n",
       "      <th>autism</th>\n",
       "      <th>country_of_res</th>\n",
       "      <th>used_app_before</th>\n",
       "      <th>age_desc</th>\n",
       "      <th>relation</th>\n",
       "      <th>Class/ASD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A1_Score  A2_Score  A3_Score  A4_Score  A5_Score  A6_Score  A7_Score  \\\n",
       "0         1         1         1         1         0         0         1   \n",
       "1         1         1         0         1         0         0         0   \n",
       "2         1         1         0         1         1         0         1   \n",
       "3         1         1         0         1         0         0         1   \n",
       "4         1         0         0         0         0         0         0   \n",
       "5         1         1         1         1         1         0         1   \n",
       "6         0         1         0         0         0         0         0   \n",
       "7         1         1         1         1         0         0         0   \n",
       "8         1         1         0         0         1         0         0   \n",
       "9         1         1         1         1         0         1         1   \n",
       "\n",
       "   A8_Score  A9_Score  A10_Score age  gender  ethnicity  jaundice  autism  \\\n",
       "0         1         0          0  26       0          9         0       0   \n",
       "1         1         0          1  24       1          5         0       1   \n",
       "2         1         1          1  27       1          5         1       1   \n",
       "3         1         0          1  35       0          9         0       1   \n",
       "4         1         0          0  40       0          9         0       0   \n",
       "5         1         1          1  36       1          6         1       0   \n",
       "6         1         0          0  17       0          3         0       0   \n",
       "7         0         1          0  64       1          9         0       0   \n",
       "8         1         1          1  29       1          9         0       0   \n",
       "9         1         1          0  17       1          2         1       1   \n",
       "\n",
       "   country_of_res  used_app_before  age_desc  relation  Class/ASD  \n",
       "0              13                0         0         4          1  \n",
       "1              30                0         0         4          1  \n",
       "2              76                0         0         2          1  \n",
       "3              13                0         0         4          0  \n",
       "4              38                0         0         4          1  \n",
       "5              13                0         0         4          0  \n",
       "6              13                0         0         4          1  \n",
       "7               4                0         0         2          0  \n",
       "8              13                0         0         4          1  \n",
       "9              24                0         0         0          1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data.rename(columns={'jundice':'jaundice','austim':'autism','contry_of_res':'country_of_res'})\n",
    "#data.drop(columns=['age_desc','contry_of_res','used_app_before','relation','ethnicity','age'],inplace=True)\n",
    "data.drop(columns=['result'],inplace=True)\n",
    "#converting into categorical values\n",
    "data['gender']=data['gender'].astype('category')\n",
    "data['gender']=data['gender'].cat.codes\n",
    "\n",
    "data['Class/ASD']=data['Class/ASD'].astype('category')\n",
    "data['Class/ASD']=data['Class/ASD'].cat.codes\n",
    "\n",
    "data['jaundice']=data['jaundice'].astype('category')\n",
    "data['jaundice']=data['jaundice'].cat.codes\n",
    "\n",
    "data['autism']=data['autism'].astype('category')\n",
    "data['autism']=data['autism'].cat.codes\n",
    "\n",
    "data['ethnicity']=data['ethnicity'].astype('category')\n",
    "data['ethnicity']=data['ethnicity'].cat.codes\n",
    "\n",
    "data['country_of_res']=data['country_of_res'].astype('category')\n",
    "data['country_of_res']=data['country_of_res'].cat.codes\n",
    "\n",
    "data['used_app_before']=data['used_app_before'].astype('category')\n",
    "data['used_app_before']=data['used_app_before'].cat.codes\n",
    "\n",
    "data['age_desc']=data['age_desc'].astype('category')\n",
    "data['age_desc']=data['age_desc'].cat.codes\n",
    "\n",
    "data['relation']=data['relation'].astype('category')\n",
    "data['relation']=data['relation'].cat.codes\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(32, input_dim=18, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(64, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 906 samples, validate on 90 samples\n",
      "Epoch 1/300\n",
      "906/906 [==============================] - 1s 1ms/step - loss: 0.2368 - acc: 0.6369 - val_loss: 0.2090 - val_acc: 0.6889\n",
      "Epoch 2/300\n",
      "906/906 [==============================] - 0s 44us/step - loss: 0.2261 - acc: 0.6369 - val_loss: 0.2072 - val_acc: 0.6889\n",
      "Epoch 3/300\n",
      "906/906 [==============================] - 0s 57us/step - loss: 0.2231 - acc: 0.6369 - val_loss: 0.2041 - val_acc: 0.6889\n",
      "Epoch 4/300\n",
      "906/906 [==============================] - 0s 55us/step - loss: 0.2198 - acc: 0.6369 - val_loss: 0.1985 - val_acc: 0.6889\n",
      "Epoch 5/300\n",
      "906/906 [==============================] - 0s 56us/step - loss: 0.2168 - acc: 0.6369 - val_loss: 0.1992 - val_acc: 0.7111\n",
      "Epoch 6/300\n",
      "906/906 [==============================] - 0s 56us/step - loss: 0.2113 - acc: 0.6832 - val_loss: 0.1899 - val_acc: 0.7556\n",
      "Epoch 7/300\n",
      "906/906 [==============================] - 0s 54us/step - loss: 0.2049 - acc: 0.6821 - val_loss: 0.1844 - val_acc: 0.7333\n",
      "Epoch 8/300\n",
      "906/906 [==============================] - 0s 63us/step - loss: 0.1982 - acc: 0.7075 - val_loss: 0.1749 - val_acc: 0.7667\n",
      "Epoch 9/300\n",
      "906/906 [==============================] - 0s 54us/step - loss: 0.1880 - acc: 0.7285 - val_loss: 0.1717 - val_acc: 0.8111\n",
      "Epoch 10/300\n",
      "906/906 [==============================] - 0s 56us/step - loss: 0.1776 - acc: 0.7693 - val_loss: 0.1571 - val_acc: 0.7778\n",
      "Epoch 11/300\n",
      "906/906 [==============================] - 0s 52us/step - loss: 0.1670 - acc: 0.7804 - val_loss: 0.1495 - val_acc: 0.8333\n",
      "Epoch 12/300\n",
      "906/906 [==============================] - 0s 50us/step - loss: 0.1558 - acc: 0.7991 - val_loss: 0.1510 - val_acc: 0.8111\n",
      "Epoch 13/300\n",
      "906/906 [==============================] - 0s 58us/step - loss: 0.1519 - acc: 0.8035 - val_loss: 0.1434 - val_acc: 0.8000\n",
      "Epoch 14/300\n",
      "906/906 [==============================] - 0s 52us/step - loss: 0.1414 - acc: 0.8212 - val_loss: 0.1381 - val_acc: 0.8111\n",
      "Epoch 15/300\n",
      "906/906 [==============================] - 0s 48us/step - loss: 0.1361 - acc: 0.8267 - val_loss: 0.1260 - val_acc: 0.8444\n",
      "Epoch 16/300\n",
      "906/906 [==============================] - 0s 51us/step - loss: 0.1269 - acc: 0.8322 - val_loss: 0.1314 - val_acc: 0.8333\n",
      "Epoch 17/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.1224 - acc: 0.8510 - val_loss: 0.1378 - val_acc: 0.8111\n",
      "Epoch 18/300\n",
      "906/906 [==============================] - 0s 51us/step - loss: 0.1225 - acc: 0.8433 - val_loss: 0.1243 - val_acc: 0.8222\n",
      "Epoch 19/300\n",
      "906/906 [==============================] - 0s 48us/step - loss: 0.1160 - acc: 0.8642 - val_loss: 0.1285 - val_acc: 0.8222\n",
      "Epoch 20/300\n",
      "906/906 [==============================] - 0s 52us/step - loss: 0.1135 - acc: 0.8576 - val_loss: 0.1159 - val_acc: 0.8222\n",
      "Epoch 21/300\n",
      "906/906 [==============================] - 0s 53us/step - loss: 0.1074 - acc: 0.8720 - val_loss: 0.1151 - val_acc: 0.8222\n",
      "Epoch 22/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.1051 - acc: 0.8698 - val_loss: 0.1492 - val_acc: 0.8111\n",
      "Epoch 23/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.1075 - acc: 0.8753 - val_loss: 0.1132 - val_acc: 0.8222\n",
      "Epoch 24/300\n",
      "906/906 [==============================] - 0s 51us/step - loss: 0.1011 - acc: 0.8742 - val_loss: 0.1265 - val_acc: 0.8111\n",
      "Epoch 25/300\n",
      "906/906 [==============================] - 0s 85us/step - loss: 0.1002 - acc: 0.8830 - val_loss: 0.1057 - val_acc: 0.8444\n",
      "Epoch 26/300\n",
      "906/906 [==============================] - 0s 66us/step - loss: 0.0969 - acc: 0.8830 - val_loss: 0.1260 - val_acc: 0.8111\n",
      "Epoch 27/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0965 - acc: 0.8786 - val_loss: 0.1111 - val_acc: 0.8444\n",
      "Epoch 28/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0988 - acc: 0.8687 - val_loss: 0.1021 - val_acc: 0.8667\n",
      "Epoch 29/300\n",
      "906/906 [==============================] - 0s 48us/step - loss: 0.0895 - acc: 0.8940 - val_loss: 0.1432 - val_acc: 0.7889\n",
      "Epoch 30/300\n",
      "906/906 [==============================] - 0s 58us/step - loss: 0.0912 - acc: 0.8951 - val_loss: 0.1351 - val_acc: 0.8111\n",
      "Epoch 31/300\n",
      "906/906 [==============================] - 0s 55us/step - loss: 0.0924 - acc: 0.8841 - val_loss: 0.1094 - val_acc: 0.8333\n",
      "Epoch 32/300\n",
      "906/906 [==============================] - 0s 53us/step - loss: 0.0877 - acc: 0.8962 - val_loss: 0.1393 - val_acc: 0.8111\n",
      "Epoch 33/300\n",
      "906/906 [==============================] - 0s 53us/step - loss: 0.0914 - acc: 0.8863 - val_loss: 0.1410 - val_acc: 0.8111\n",
      "Epoch 34/300\n",
      "906/906 [==============================] - 0s 50us/step - loss: 0.0900 - acc: 0.8896 - val_loss: 0.0946 - val_acc: 0.8889\n",
      "Epoch 35/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0866 - acc: 0.8985 - val_loss: 0.0958 - val_acc: 0.8667\n",
      "Epoch 36/300\n",
      "906/906 [==============================] - 0s 50us/step - loss: 0.0843 - acc: 0.9007 - val_loss: 0.0915 - val_acc: 0.8778\n",
      "Epoch 37/300\n",
      "906/906 [==============================] - 0s 48us/step - loss: 0.0884 - acc: 0.8907 - val_loss: 0.0910 - val_acc: 0.8778\n",
      "Epoch 38/300\n",
      "906/906 [==============================] - 0s 48us/step - loss: 0.0807 - acc: 0.9084 - val_loss: 0.0881 - val_acc: 0.8667\n",
      "Epoch 39/300\n",
      "906/906 [==============================] - 0s 42us/step - loss: 0.0838 - acc: 0.9040 - val_loss: 0.1022 - val_acc: 0.8556\n",
      "Epoch 40/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0813 - acc: 0.9062 - val_loss: 0.1256 - val_acc: 0.8222\n",
      "Epoch 41/300\n",
      "906/906 [==============================] - 0s 55us/step - loss: 0.0832 - acc: 0.8985 - val_loss: 0.0901 - val_acc: 0.8778\n",
      "Epoch 42/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0813 - acc: 0.9018 - val_loss: 0.1058 - val_acc: 0.8333\n",
      "Epoch 43/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0786 - acc: 0.9106 - val_loss: 0.1029 - val_acc: 0.8444\n",
      "Epoch 44/300\n",
      "906/906 [==============================] - 0s 50us/step - loss: 0.0782 - acc: 0.9095 - val_loss: 0.1146 - val_acc: 0.8444\n",
      "Epoch 45/300\n",
      "906/906 [==============================] - 0s 51us/step - loss: 0.0767 - acc: 0.9029 - val_loss: 0.0857 - val_acc: 0.8778\n",
      "Epoch 46/300\n",
      "906/906 [==============================] - 0s 51us/step - loss: 0.0803 - acc: 0.9007 - val_loss: 0.0843 - val_acc: 0.8667\n",
      "Epoch 47/300\n",
      "906/906 [==============================] - 0s 50us/step - loss: 0.0759 - acc: 0.9117 - val_loss: 0.1020 - val_acc: 0.8444\n",
      "Epoch 48/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0767 - acc: 0.9095 - val_loss: 0.0886 - val_acc: 0.8667\n",
      "Epoch 49/300\n",
      "906/906 [==============================] - 0s 44us/step - loss: 0.0784 - acc: 0.9029 - val_loss: 0.0965 - val_acc: 0.8556\n",
      "Epoch 50/300\n",
      "906/906 [==============================] - 0s 52us/step - loss: 0.0741 - acc: 0.9183 - val_loss: 0.1347 - val_acc: 0.8222\n",
      "Epoch 51/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0761 - acc: 0.9106 - val_loss: 0.0980 - val_acc: 0.8667\n",
      "Epoch 52/300\n",
      "906/906 [==============================] - 0s 51us/step - loss: 0.0732 - acc: 0.9139 - val_loss: 0.0959 - val_acc: 0.8667\n",
      "Epoch 53/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0748 - acc: 0.9106 - val_loss: 0.0933 - val_acc: 0.8667\n",
      "Epoch 54/300\n",
      "906/906 [==============================] - 0s 48us/step - loss: 0.0695 - acc: 0.9216 - val_loss: 0.1121 - val_acc: 0.8444\n",
      "Epoch 55/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0758 - acc: 0.9062 - val_loss: 0.0891 - val_acc: 0.8778\n",
      "Epoch 56/300\n",
      "906/906 [==============================] - 0s 48us/step - loss: 0.0722 - acc: 0.9095 - val_loss: 0.0911 - val_acc: 0.8667\n",
      "Epoch 57/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0701 - acc: 0.9172 - val_loss: 0.0807 - val_acc: 0.8778\n",
      "Epoch 58/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0676 - acc: 0.9183 - val_loss: 0.1148 - val_acc: 0.8444\n",
      "Epoch 59/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0721 - acc: 0.9172 - val_loss: 0.0798 - val_acc: 0.8667\n",
      "Epoch 60/300\n",
      "906/906 [==============================] - 0s 48us/step - loss: 0.0699 - acc: 0.9183 - val_loss: 0.0861 - val_acc: 0.8778\n",
      "Epoch 61/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "906/906 [==============================] - 0s 48us/step - loss: 0.0685 - acc: 0.9227 - val_loss: 0.1360 - val_acc: 0.8111\n",
      "Epoch 62/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0740 - acc: 0.9117 - val_loss: 0.0790 - val_acc: 0.8889\n",
      "Epoch 63/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0657 - acc: 0.9260 - val_loss: 0.0898 - val_acc: 0.8778\n",
      "Epoch 64/300\n",
      "906/906 [==============================] - 0s 52us/step - loss: 0.0673 - acc: 0.9216 - val_loss: 0.0783 - val_acc: 0.8778\n",
      "Epoch 65/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0674 - acc: 0.9150 - val_loss: 0.0819 - val_acc: 0.8889\n",
      "Epoch 66/300\n",
      "906/906 [==============================] - 0s 50us/step - loss: 0.0673 - acc: 0.9205 - val_loss: 0.1311 - val_acc: 0.8222\n",
      "Epoch 67/300\n",
      "906/906 [==============================] - 0s 56us/step - loss: 0.0688 - acc: 0.9150 - val_loss: 0.1131 - val_acc: 0.8444\n",
      "Epoch 68/300\n",
      "906/906 [==============================] - 0s 58us/step - loss: 0.0677 - acc: 0.9205 - val_loss: 0.1642 - val_acc: 0.8000\n",
      "Epoch 69/300\n",
      "906/906 [==============================] - 0s 61us/step - loss: 0.0730 - acc: 0.9128 - val_loss: 0.1376 - val_acc: 0.8111\n",
      "Epoch 70/300\n",
      "906/906 [==============================] - 0s 56us/step - loss: 0.0662 - acc: 0.9227 - val_loss: 0.0912 - val_acc: 0.8556\n",
      "Epoch 71/300\n",
      "906/906 [==============================] - 0s 53us/step - loss: 0.0644 - acc: 0.9283 - val_loss: 0.0743 - val_acc: 0.9111\n",
      "Epoch 72/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0654 - acc: 0.9260 - val_loss: 0.0782 - val_acc: 0.8667\n",
      "Epoch 73/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0610 - acc: 0.9327 - val_loss: 0.0736 - val_acc: 0.9000\n",
      "Epoch 74/300\n",
      "906/906 [==============================] - 0s 52us/step - loss: 0.0666 - acc: 0.9227 - val_loss: 0.0921 - val_acc: 0.8667\n",
      "Epoch 75/300\n",
      "906/906 [==============================] - 0s 52us/step - loss: 0.0665 - acc: 0.9227 - val_loss: 0.0792 - val_acc: 0.8778\n",
      "Epoch 76/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0615 - acc: 0.9349 - val_loss: 0.0816 - val_acc: 0.8778\n",
      "Epoch 77/300\n",
      "906/906 [==============================] - 0s 50us/step - loss: 0.0609 - acc: 0.9360 - val_loss: 0.0744 - val_acc: 0.8778\n",
      "Epoch 78/300\n",
      "906/906 [==============================] - 0s 50us/step - loss: 0.0696 - acc: 0.9183 - val_loss: 0.0942 - val_acc: 0.8667\n",
      "Epoch 79/300\n",
      "906/906 [==============================] - 0s 48us/step - loss: 0.0600 - acc: 0.9360 - val_loss: 0.1282 - val_acc: 0.8333\n",
      "Epoch 80/300\n",
      "906/906 [==============================] - 0s 51us/step - loss: 0.0653 - acc: 0.9260 - val_loss: 0.0718 - val_acc: 0.9000\n",
      "Epoch 81/300\n",
      "906/906 [==============================] - 0s 52us/step - loss: 0.0593 - acc: 0.9338 - val_loss: 0.0858 - val_acc: 0.8889\n",
      "Epoch 82/300\n",
      "906/906 [==============================] - 0s 50us/step - loss: 0.0577 - acc: 0.9404 - val_loss: 0.0745 - val_acc: 0.8889\n",
      "Epoch 83/300\n",
      "906/906 [==============================] - 0s 51us/step - loss: 0.0596 - acc: 0.9382 - val_loss: 0.1108 - val_acc: 0.8556\n",
      "Epoch 84/300\n",
      "906/906 [==============================] - 0s 48us/step - loss: 0.0640 - acc: 0.9238 - val_loss: 0.0809 - val_acc: 0.9000\n",
      "Epoch 85/300\n",
      "906/906 [==============================] - 0s 57us/step - loss: 0.0605 - acc: 0.9316 - val_loss: 0.0724 - val_acc: 0.8889\n",
      "Epoch 86/300\n",
      "906/906 [==============================] - 0s 61us/step - loss: 0.0567 - acc: 0.9371 - val_loss: 0.0933 - val_acc: 0.8778\n",
      "Epoch 87/300\n",
      "906/906 [==============================] - 0s 50us/step - loss: 0.0602 - acc: 0.9338 - val_loss: 0.1027 - val_acc: 0.8667\n",
      "Epoch 88/300\n",
      "906/906 [==============================] - 0s 50us/step - loss: 0.0578 - acc: 0.9371 - val_loss: 0.0698 - val_acc: 0.9000\n",
      "Epoch 89/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0594 - acc: 0.9371 - val_loss: 0.1040 - val_acc: 0.8333\n",
      "Epoch 90/300\n",
      "906/906 [==============================] - 0s 52us/step - loss: 0.0552 - acc: 0.9393 - val_loss: 0.0729 - val_acc: 0.8889\n",
      "Epoch 91/300\n",
      "906/906 [==============================] - 0s 56us/step - loss: 0.0572 - acc: 0.9360 - val_loss: 0.0829 - val_acc: 0.8778\n",
      "Epoch 92/300\n",
      "906/906 [==============================] - 0s 51us/step - loss: 0.0602 - acc: 0.9305 - val_loss: 0.1032 - val_acc: 0.8667\n",
      "Epoch 93/300\n",
      "906/906 [==============================] - 0s 56us/step - loss: 0.0636 - acc: 0.9238 - val_loss: 0.0690 - val_acc: 0.9111\n",
      "Epoch 94/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0575 - acc: 0.9360 - val_loss: 0.0700 - val_acc: 0.9000\n",
      "Epoch 95/300\n",
      "906/906 [==============================] - 0s 51us/step - loss: 0.0569 - acc: 0.9294 - val_loss: 0.0938 - val_acc: 0.8667\n",
      "Epoch 96/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0581 - acc: 0.9338 - val_loss: 0.0727 - val_acc: 0.8778\n",
      "Epoch 97/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0604 - acc: 0.9260 - val_loss: 0.0734 - val_acc: 0.8889\n",
      "Epoch 98/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0542 - acc: 0.9415 - val_loss: 0.0882 - val_acc: 0.8778\n",
      "Epoch 99/300\n",
      "906/906 [==============================] - 0s 44us/step - loss: 0.0581 - acc: 0.9349 - val_loss: 0.0897 - val_acc: 0.8889\n",
      "Epoch 100/300\n",
      "906/906 [==============================] - 0s 43us/step - loss: 0.0550 - acc: 0.9481 - val_loss: 0.1225 - val_acc: 0.8333\n",
      "Epoch 101/300\n",
      "906/906 [==============================] - 0s 44us/step - loss: 0.0572 - acc: 0.9404 - val_loss: 0.0827 - val_acc: 0.9000\n",
      "Epoch 102/300\n",
      "906/906 [==============================] - 0s 51us/step - loss: 0.0541 - acc: 0.9393 - val_loss: 0.0770 - val_acc: 0.9000\n",
      "Epoch 103/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0595 - acc: 0.9294 - val_loss: 0.0794 - val_acc: 0.8889\n",
      "Epoch 104/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0536 - acc: 0.9393 - val_loss: 0.0678 - val_acc: 0.8889\n",
      "Epoch 105/300\n",
      "906/906 [==============================] - 0s 48us/step - loss: 0.0591 - acc: 0.9294 - val_loss: 0.0826 - val_acc: 0.8778\n",
      "Epoch 106/300\n",
      "906/906 [==============================] - 0s 43us/step - loss: 0.0552 - acc: 0.9393 - val_loss: 0.0753 - val_acc: 0.8778\n",
      "Epoch 107/300\n",
      "906/906 [==============================] - 0s 42us/step - loss: 0.0541 - acc: 0.9437 - val_loss: 0.1044 - val_acc: 0.8667\n",
      "Epoch 108/300\n",
      "906/906 [==============================] - 0s 43us/step - loss: 0.0593 - acc: 0.9260 - val_loss: 0.0840 - val_acc: 0.9000\n",
      "Epoch 109/300\n",
      "906/906 [==============================] - 0s 52us/step - loss: 0.0532 - acc: 0.9415 - val_loss: 0.0775 - val_acc: 0.9000\n",
      "Epoch 110/300\n",
      "906/906 [==============================] - 0s 49us/step - loss: 0.0510 - acc: 0.9459 - val_loss: 0.0675 - val_acc: 0.9000\n",
      "Epoch 111/300\n",
      "906/906 [==============================] - 0s 44us/step - loss: 0.0531 - acc: 0.9437 - val_loss: 0.0658 - val_acc: 0.9000\n",
      "Epoch 112/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0581 - acc: 0.9316 - val_loss: 0.0637 - val_acc: 0.9222\n",
      "Epoch 113/300\n",
      "906/906 [==============================] - 0s 44us/step - loss: 0.0506 - acc: 0.9481 - val_loss: 0.0761 - val_acc: 0.9000\n",
      "Epoch 114/300\n",
      "906/906 [==============================] - 0s 48us/step - loss: 0.0537 - acc: 0.9426 - val_loss: 0.0646 - val_acc: 0.9111\n",
      "Epoch 115/300\n",
      "906/906 [==============================] - 0s 44us/step - loss: 0.0564 - acc: 0.9338 - val_loss: 0.0749 - val_acc: 0.9000\n",
      "Epoch 116/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0512 - acc: 0.9437 - val_loss: 0.0688 - val_acc: 0.8889\n",
      "Epoch 117/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0560 - acc: 0.9371 - val_loss: 0.0665 - val_acc: 0.9000\n",
      "Epoch 118/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0516 - acc: 0.9437 - val_loss: 0.0659 - val_acc: 0.8778\n",
      "Epoch 119/300\n",
      "906/906 [==============================] - 0s 44us/step - loss: 0.0520 - acc: 0.9393 - val_loss: 0.0682 - val_acc: 0.9000\n",
      "Epoch 120/300\n",
      "906/906 [==============================] - 0s 44us/step - loss: 0.0548 - acc: 0.9426 - val_loss: 0.0743 - val_acc: 0.9000\n",
      "Epoch 121/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "906/906 [==============================] - 0s 45us/step - loss: 0.0514 - acc: 0.9415 - val_loss: 0.1009 - val_acc: 0.8444\n",
      "Epoch 122/300\n",
      "906/906 [==============================] - 0s 44us/step - loss: 0.0554 - acc: 0.9382 - val_loss: 0.0655 - val_acc: 0.9000\n",
      "Epoch 123/300\n",
      "906/906 [==============================] - 0s 43us/step - loss: 0.0511 - acc: 0.9426 - val_loss: 0.0647 - val_acc: 0.9222\n",
      "Epoch 124/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0540 - acc: 0.9349 - val_loss: 0.1080 - val_acc: 0.8556\n",
      "Epoch 125/300\n",
      "906/906 [==============================] - 0s 44us/step - loss: 0.0547 - acc: 0.9404 - val_loss: 0.0692 - val_acc: 0.9111\n",
      "Epoch 126/300\n",
      "906/906 [==============================] - 0s 56us/step - loss: 0.0472 - acc: 0.9503 - val_loss: 0.1230 - val_acc: 0.8444\n",
      "Epoch 127/300\n",
      "906/906 [==============================] - 0s 44us/step - loss: 0.0536 - acc: 0.9404 - val_loss: 0.0663 - val_acc: 0.9000\n",
      "Epoch 128/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0517 - acc: 0.9382 - val_loss: 0.0663 - val_acc: 0.9111\n",
      "Epoch 129/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0481 - acc: 0.9503 - val_loss: 0.0635 - val_acc: 0.9111\n",
      "Epoch 130/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0537 - acc: 0.9426 - val_loss: 0.0650 - val_acc: 0.9000\n",
      "Epoch 131/300\n",
      "906/906 [==============================] - 0s 50us/step - loss: 0.0554 - acc: 0.9305 - val_loss: 0.0832 - val_acc: 0.9000\n",
      "Epoch 132/300\n",
      "906/906 [==============================] - 0s 84us/step - loss: 0.0500 - acc: 0.9437 - val_loss: 0.1019 - val_acc: 0.8556\n",
      "Epoch 133/300\n",
      "906/906 [==============================] - 0s 55us/step - loss: 0.0550 - acc: 0.9360 - val_loss: 0.0650 - val_acc: 0.9111\n",
      "Epoch 134/300\n",
      "906/906 [==============================] - 0s 52us/step - loss: 0.0518 - acc: 0.9404 - val_loss: 0.0603 - val_acc: 0.9111\n",
      "Epoch 135/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0491 - acc: 0.9437 - val_loss: 0.0657 - val_acc: 0.9111\n",
      "Epoch 136/300\n",
      "906/906 [==============================] - 0s 43us/step - loss: 0.0528 - acc: 0.9404 - val_loss: 0.1117 - val_acc: 0.8556\n",
      "Epoch 137/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0508 - acc: 0.9426 - val_loss: 0.0634 - val_acc: 0.9000\n",
      "Epoch 138/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0468 - acc: 0.9514 - val_loss: 0.0794 - val_acc: 0.9000\n",
      "Epoch 139/300\n",
      "906/906 [==============================] - 0s 44us/step - loss: 0.0535 - acc: 0.9382 - val_loss: 0.0615 - val_acc: 0.9111\n",
      "Epoch 140/300\n",
      "906/906 [==============================] - 0s 42us/step - loss: 0.0506 - acc: 0.9481 - val_loss: 0.0634 - val_acc: 0.9222\n",
      "Epoch 141/300\n",
      "906/906 [==============================] - 0s 54us/step - loss: 0.0477 - acc: 0.9492 - val_loss: 0.0757 - val_acc: 0.9000\n",
      "Epoch 142/300\n",
      "906/906 [==============================] - 0s 55us/step - loss: 0.0508 - acc: 0.9426 - val_loss: 0.1081 - val_acc: 0.8556\n",
      "Epoch 143/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0518 - acc: 0.9459 - val_loss: 0.0604 - val_acc: 0.9111\n",
      "Epoch 144/300\n",
      "906/906 [==============================] - 0s 48us/step - loss: 0.0479 - acc: 0.9448 - val_loss: 0.0603 - val_acc: 0.9111\n",
      "Epoch 145/300\n",
      "906/906 [==============================] - 0s 48us/step - loss: 0.0484 - acc: 0.9492 - val_loss: 0.1326 - val_acc: 0.8444\n",
      "Epoch 146/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0540 - acc: 0.9437 - val_loss: 0.0590 - val_acc: 0.9222\n",
      "Epoch 147/300\n",
      "906/906 [==============================] - 0s 54us/step - loss: 0.0482 - acc: 0.9492 - val_loss: 0.0764 - val_acc: 0.9000\n",
      "Epoch 148/300\n",
      "906/906 [==============================] - 0s 58us/step - loss: 0.0488 - acc: 0.9470 - val_loss: 0.1538 - val_acc: 0.8000\n",
      "Epoch 149/300\n",
      "906/906 [==============================] - 0s 41us/step - loss: 0.0596 - acc: 0.9260 - val_loss: 0.0626 - val_acc: 0.9111\n",
      "Epoch 150/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0449 - acc: 0.9514 - val_loss: 0.0658 - val_acc: 0.9111\n",
      "Epoch 151/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0445 - acc: 0.9536 - val_loss: 0.0623 - val_acc: 0.9111\n",
      "Epoch 152/300\n",
      "906/906 [==============================] - 0s 42us/step - loss: 0.0582 - acc: 0.9327 - val_loss: 0.0661 - val_acc: 0.9222\n",
      "Epoch 153/300\n",
      "906/906 [==============================] - 0s 52us/step - loss: 0.0515 - acc: 0.9382 - val_loss: 0.0870 - val_acc: 0.8889\n",
      "Epoch 154/300\n",
      "906/906 [==============================] - 0s 51us/step - loss: 0.0465 - acc: 0.9470 - val_loss: 0.0643 - val_acc: 0.9111\n",
      "Epoch 155/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0432 - acc: 0.9536 - val_loss: 0.1298 - val_acc: 0.8556\n",
      "Epoch 156/300\n",
      "906/906 [==============================] - 0s 51us/step - loss: 0.0519 - acc: 0.9415 - val_loss: 0.0608 - val_acc: 0.9111\n",
      "Epoch 157/300\n",
      "906/906 [==============================] - 0s 44us/step - loss: 0.0447 - acc: 0.9525 - val_loss: 0.0669 - val_acc: 0.9222\n",
      "Epoch 158/300\n",
      "906/906 [==============================] - 0s 57us/step - loss: 0.0484 - acc: 0.9459 - val_loss: 0.0652 - val_acc: 0.9222\n",
      "Epoch 159/300\n",
      "906/906 [==============================] - 0s 52us/step - loss: 0.0475 - acc: 0.9481 - val_loss: 0.0584 - val_acc: 0.9111\n",
      "Epoch 160/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0454 - acc: 0.9503 - val_loss: 0.1063 - val_acc: 0.8667\n",
      "Epoch 161/300\n",
      "906/906 [==============================] - 0s 48us/step - loss: 0.0512 - acc: 0.9426 - val_loss: 0.0625 - val_acc: 0.9111\n",
      "Epoch 162/300\n",
      "906/906 [==============================] - 0s 48us/step - loss: 0.0511 - acc: 0.9393 - val_loss: 0.0611 - val_acc: 0.9333\n",
      "Epoch 163/300\n",
      "906/906 [==============================] - 0s 50us/step - loss: 0.0431 - acc: 0.9558 - val_loss: 0.0940 - val_acc: 0.8778\n",
      "Epoch 164/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0466 - acc: 0.9481 - val_loss: 0.1174 - val_acc: 0.8556\n",
      "Epoch 165/300\n",
      "906/906 [==============================] - 0s 43us/step - loss: 0.0466 - acc: 0.9459 - val_loss: 0.0656 - val_acc: 0.9222\n",
      "Epoch 166/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0445 - acc: 0.9536 - val_loss: 0.0591 - val_acc: 0.9222\n",
      "Epoch 167/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0440 - acc: 0.9525 - val_loss: 0.0628 - val_acc: 0.9111\n",
      "Epoch 168/300\n",
      "906/906 [==============================] - 0s 44us/step - loss: 0.0473 - acc: 0.9437 - val_loss: 0.0902 - val_acc: 0.8778\n",
      "Epoch 169/300\n",
      "906/906 [==============================] - 0s 49us/step - loss: 0.0520 - acc: 0.9448 - val_loss: 0.0595 - val_acc: 0.9000\n",
      "Epoch 170/300\n",
      "906/906 [==============================] - 0s 52us/step - loss: 0.0417 - acc: 0.9581 - val_loss: 0.0669 - val_acc: 0.8889\n",
      "Epoch 171/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0536 - acc: 0.9371 - val_loss: 0.0615 - val_acc: 0.9111\n",
      "Epoch 172/300\n",
      "906/906 [==============================] - 0s 44us/step - loss: 0.0452 - acc: 0.9503 - val_loss: 0.0910 - val_acc: 0.8889\n",
      "Epoch 173/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0534 - acc: 0.9415 - val_loss: 0.0708 - val_acc: 0.9111\n",
      "Epoch 174/300\n",
      "906/906 [==============================] - 0s 41us/step - loss: 0.0463 - acc: 0.9525 - val_loss: 0.0808 - val_acc: 0.8889\n",
      "Epoch 175/300\n",
      "906/906 [==============================] - 0s 44us/step - loss: 0.0458 - acc: 0.9492 - val_loss: 0.0626 - val_acc: 0.9111\n",
      "Epoch 176/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0458 - acc: 0.9503 - val_loss: 0.1247 - val_acc: 0.8556\n",
      "Epoch 177/300\n",
      "906/906 [==============================] - 0s 44us/step - loss: 0.0488 - acc: 0.9437 - val_loss: 0.0631 - val_acc: 0.9111\n",
      "Epoch 178/300\n",
      "906/906 [==============================] - 0s 43us/step - loss: 0.0430 - acc: 0.9547 - val_loss: 0.0594 - val_acc: 0.9000\n",
      "Epoch 179/300\n",
      "906/906 [==============================] - 0s 48us/step - loss: 0.0461 - acc: 0.9503 - val_loss: 0.0583 - val_acc: 0.9333\n",
      "Epoch 180/300\n",
      "906/906 [==============================] - 0s 42us/step - loss: 0.0438 - acc: 0.9547 - val_loss: 0.1190 - val_acc: 0.8667\n",
      "Epoch 181/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "906/906 [==============================] - 0s 43us/step - loss: 0.0497 - acc: 0.9459 - val_loss: 0.0691 - val_acc: 0.9111\n",
      "Epoch 182/300\n",
      "906/906 [==============================] - 0s 48us/step - loss: 0.0432 - acc: 0.9558 - val_loss: 0.1281 - val_acc: 0.8556\n",
      "Epoch 183/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0494 - acc: 0.9503 - val_loss: 0.0819 - val_acc: 0.8889\n",
      "Epoch 184/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0478 - acc: 0.9459 - val_loss: 0.0607 - val_acc: 0.9111\n",
      "Epoch 185/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0454 - acc: 0.9503 - val_loss: 0.0637 - val_acc: 0.9111\n",
      "Epoch 186/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0420 - acc: 0.9525 - val_loss: 0.0669 - val_acc: 0.9000\n",
      "Epoch 187/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0433 - acc: 0.9547 - val_loss: 0.0801 - val_acc: 0.8889\n",
      "Epoch 188/300\n",
      "906/906 [==============================] - 0s 52us/step - loss: 0.0491 - acc: 0.9459 - val_loss: 0.0667 - val_acc: 0.9111\n",
      "Epoch 189/300\n",
      "906/906 [==============================] - 0s 44us/step - loss: 0.0427 - acc: 0.9570 - val_loss: 0.0682 - val_acc: 0.9000\n",
      "Epoch 190/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0500 - acc: 0.9437 - val_loss: 0.0682 - val_acc: 0.8889\n",
      "Epoch 191/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0437 - acc: 0.9525 - val_loss: 0.0818 - val_acc: 0.8889\n",
      "Epoch 192/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0439 - acc: 0.9536 - val_loss: 0.0866 - val_acc: 0.8889\n",
      "Epoch 193/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0494 - acc: 0.9393 - val_loss: 0.0674 - val_acc: 0.9000\n",
      "Epoch 194/300\n",
      "906/906 [==============================] - 0s 49us/step - loss: 0.0407 - acc: 0.9592 - val_loss: 0.0752 - val_acc: 0.9000\n",
      "Epoch 195/300\n",
      "906/906 [==============================] - 0s 50us/step - loss: 0.0414 - acc: 0.9581 - val_loss: 0.0636 - val_acc: 0.9000\n",
      "Epoch 196/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0437 - acc: 0.9558 - val_loss: 0.0631 - val_acc: 0.9111\n",
      "Epoch 197/300\n",
      "906/906 [==============================] - 0s 43us/step - loss: 0.0461 - acc: 0.9481 - val_loss: 0.0772 - val_acc: 0.9000\n",
      "Epoch 198/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0511 - acc: 0.9415 - val_loss: 0.0658 - val_acc: 0.9000\n",
      "Epoch 199/300\n",
      "906/906 [==============================] - 0s 54us/step - loss: 0.0402 - acc: 0.9570 - val_loss: 0.0630 - val_acc: 0.9000\n",
      "Epoch 200/300\n",
      "906/906 [==============================] - 0s 52us/step - loss: 0.0413 - acc: 0.9547 - val_loss: 0.0655 - val_acc: 0.9000\n",
      "Epoch 201/300\n",
      "906/906 [==============================] - 0s 48us/step - loss: 0.0489 - acc: 0.9448 - val_loss: 0.0614 - val_acc: 0.9000\n",
      "Epoch 202/300\n",
      "906/906 [==============================] - 0s 53us/step - loss: 0.0412 - acc: 0.9592 - val_loss: 0.0662 - val_acc: 0.9111\n",
      "Epoch 203/300\n",
      "906/906 [==============================] - 0s 56us/step - loss: 0.0466 - acc: 0.9503 - val_loss: 0.0881 - val_acc: 0.8889\n",
      "Epoch 204/300\n",
      "906/906 [==============================] - 0s 44us/step - loss: 0.0427 - acc: 0.9525 - val_loss: 0.0600 - val_acc: 0.9222\n",
      "Epoch 205/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0409 - acc: 0.9592 - val_loss: 0.0603 - val_acc: 0.9222\n",
      "Epoch 206/300\n",
      "906/906 [==============================] - 0s 55us/step - loss: 0.0421 - acc: 0.9592 - val_loss: 0.1162 - val_acc: 0.8556\n",
      "Epoch 207/300\n",
      "906/906 [==============================] - 0s 50us/step - loss: 0.0495 - acc: 0.9448 - val_loss: 0.0655 - val_acc: 0.9222\n",
      "Epoch 208/300\n",
      "906/906 [==============================] - 0s 43us/step - loss: 0.0438 - acc: 0.9570 - val_loss: 0.0649 - val_acc: 0.9111\n",
      "Epoch 209/300\n",
      "906/906 [==============================] - 0s 53us/step - loss: 0.0416 - acc: 0.9592 - val_loss: 0.0691 - val_acc: 0.9111\n",
      "Epoch 210/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0442 - acc: 0.9514 - val_loss: 0.1010 - val_acc: 0.8889\n",
      "Epoch 211/300\n",
      "906/906 [==============================] - 0s 44us/step - loss: 0.0471 - acc: 0.9470 - val_loss: 0.0844 - val_acc: 0.9000\n",
      "Epoch 212/300\n",
      "906/906 [==============================] - 0s 50us/step - loss: 0.0422 - acc: 0.9525 - val_loss: 0.0657 - val_acc: 0.9111\n",
      "Epoch 213/300\n",
      "906/906 [==============================] - 0s 44us/step - loss: 0.0548 - acc: 0.9382 - val_loss: 0.0670 - val_acc: 0.8778\n",
      "Epoch 214/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0390 - acc: 0.9603 - val_loss: 0.0686 - val_acc: 0.9000\n",
      "Epoch 215/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0426 - acc: 0.9536 - val_loss: 0.0939 - val_acc: 0.8667\n",
      "Epoch 216/300\n",
      "906/906 [==============================] - 0s 50us/step - loss: 0.0419 - acc: 0.9558 - val_loss: 0.0646 - val_acc: 0.9000\n",
      "Epoch 217/300\n",
      "906/906 [==============================] - 0s 51us/step - loss: 0.0384 - acc: 0.9625 - val_loss: 0.0685 - val_acc: 0.9000\n",
      "Epoch 218/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0427 - acc: 0.9525 - val_loss: 0.0659 - val_acc: 0.9111\n",
      "Epoch 219/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0464 - acc: 0.9470 - val_loss: 0.1027 - val_acc: 0.8889\n",
      "Epoch 220/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0402 - acc: 0.9592 - val_loss: 0.0735 - val_acc: 0.9000\n",
      "Epoch 221/300\n",
      "906/906 [==============================] - 0s 43us/step - loss: 0.0473 - acc: 0.9448 - val_loss: 0.1007 - val_acc: 0.8778\n",
      "Epoch 222/300\n",
      "906/906 [==============================] - 0s 49us/step - loss: 0.0474 - acc: 0.9448 - val_loss: 0.0729 - val_acc: 0.9111\n",
      "Epoch 223/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0417 - acc: 0.9592 - val_loss: 0.0979 - val_acc: 0.8889\n",
      "Epoch 224/300\n",
      "906/906 [==============================] - 0s 42us/step - loss: 0.0423 - acc: 0.9547 - val_loss: 0.0743 - val_acc: 0.9111\n",
      "Epoch 225/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0389 - acc: 0.9603 - val_loss: 0.0731 - val_acc: 0.8889\n",
      "Epoch 226/300\n",
      "906/906 [==============================] - 0s 48us/step - loss: 0.0425 - acc: 0.9581 - val_loss: 0.0898 - val_acc: 0.8889\n",
      "Epoch 227/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0462 - acc: 0.9448 - val_loss: 0.0692 - val_acc: 0.9111\n",
      "Epoch 228/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0412 - acc: 0.9614 - val_loss: 0.0765 - val_acc: 0.8889\n",
      "Epoch 229/300\n",
      "906/906 [==============================] - 0s 42us/step - loss: 0.0410 - acc: 0.9525 - val_loss: 0.0722 - val_acc: 0.8889\n",
      "Epoch 230/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0437 - acc: 0.9536 - val_loss: 0.0689 - val_acc: 0.9000\n",
      "Epoch 231/300\n",
      "906/906 [==============================] - 0s 44us/step - loss: 0.0407 - acc: 0.9603 - val_loss: 0.0664 - val_acc: 0.9111\n",
      "Epoch 232/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0396 - acc: 0.9592 - val_loss: 0.1021 - val_acc: 0.8667\n",
      "Epoch 233/300\n",
      "906/906 [==============================] - 0s 51us/step - loss: 0.0438 - acc: 0.9514 - val_loss: 0.0671 - val_acc: 0.9000\n",
      "Epoch 234/300\n",
      "906/906 [==============================] - 0s 48us/step - loss: 0.0437 - acc: 0.9536 - val_loss: 0.0707 - val_acc: 0.9111\n",
      "Epoch 235/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0431 - acc: 0.9536 - val_loss: 0.1026 - val_acc: 0.8556\n",
      "Epoch 236/300\n",
      "906/906 [==============================] - 0s 53us/step - loss: 0.0511 - acc: 0.9404 - val_loss: 0.0702 - val_acc: 0.8889\n",
      "Epoch 237/300\n",
      "906/906 [==============================] - 0s 43us/step - loss: 0.0397 - acc: 0.9592 - val_loss: 0.0789 - val_acc: 0.8889\n",
      "Epoch 238/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0484 - acc: 0.9459 - val_loss: 0.0702 - val_acc: 0.8889\n",
      "Epoch 239/300\n",
      "906/906 [==============================] - 0s 48us/step - loss: 0.0369 - acc: 0.9636 - val_loss: 0.0712 - val_acc: 0.9111\n",
      "Epoch 240/300\n",
      "906/906 [==============================] - 0s 48us/step - loss: 0.0450 - acc: 0.9492 - val_loss: 0.0690 - val_acc: 0.9111\n",
      "Epoch 241/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "906/906 [==============================] - 0s 53us/step - loss: 0.0415 - acc: 0.9547 - val_loss: 0.0686 - val_acc: 0.9111\n",
      "Epoch 242/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0455 - acc: 0.9514 - val_loss: 0.1197 - val_acc: 0.8556\n",
      "Epoch 243/300\n",
      "906/906 [==============================] - 0s 53us/step - loss: 0.0438 - acc: 0.9558 - val_loss: 0.0732 - val_acc: 0.9000\n",
      "Epoch 244/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0394 - acc: 0.9592 - val_loss: 0.0783 - val_acc: 0.8778\n",
      "Epoch 245/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0382 - acc: 0.9625 - val_loss: 0.0680 - val_acc: 0.9000\n",
      "Epoch 246/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0430 - acc: 0.9558 - val_loss: 0.0693 - val_acc: 0.8889\n",
      "Epoch 247/300\n",
      "906/906 [==============================] - 0s 43us/step - loss: 0.0376 - acc: 0.9592 - val_loss: 0.0902 - val_acc: 0.8889\n",
      "Epoch 248/300\n",
      "906/906 [==============================] - 0s 51us/step - loss: 0.0438 - acc: 0.9525 - val_loss: 0.0934 - val_acc: 0.8778\n",
      "Epoch 249/300\n",
      "906/906 [==============================] - 0s 52us/step - loss: 0.0412 - acc: 0.9558 - val_loss: 0.0798 - val_acc: 0.9000\n",
      "Epoch 250/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0414 - acc: 0.9558 - val_loss: 0.0719 - val_acc: 0.9111\n",
      "Epoch 251/300\n",
      "906/906 [==============================] - 0s 48us/step - loss: 0.0431 - acc: 0.9536 - val_loss: 0.1073 - val_acc: 0.8778\n",
      "Epoch 252/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0480 - acc: 0.9437 - val_loss: 0.0837 - val_acc: 0.8889\n",
      "Epoch 253/300\n",
      "906/906 [==============================] - 0s 51us/step - loss: 0.0382 - acc: 0.9614 - val_loss: 0.0756 - val_acc: 0.9000\n",
      "Epoch 254/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0378 - acc: 0.9625 - val_loss: 0.0756 - val_acc: 0.9000\n",
      "Epoch 255/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0376 - acc: 0.9625 - val_loss: 0.1125 - val_acc: 0.8556\n",
      "Epoch 256/300\n",
      "906/906 [==============================] - 0s 48us/step - loss: 0.0462 - acc: 0.9503 - val_loss: 0.0736 - val_acc: 0.8889\n",
      "Epoch 257/300\n",
      "906/906 [==============================] - 0s 51us/step - loss: 0.0371 - acc: 0.9636 - val_loss: 0.0745 - val_acc: 0.8889\n",
      "Epoch 258/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0375 - acc: 0.9647 - val_loss: 0.0872 - val_acc: 0.8889\n",
      "Epoch 259/300\n",
      "906/906 [==============================] - 0s 48us/step - loss: 0.0437 - acc: 0.9536 - val_loss: 0.0990 - val_acc: 0.8889\n",
      "Epoch 260/300\n",
      "906/906 [==============================] - 0s 48us/step - loss: 0.0430 - acc: 0.9581 - val_loss: 0.0790 - val_acc: 0.8889\n",
      "Epoch 261/300\n",
      "906/906 [==============================] - 0s 52us/step - loss: 0.0391 - acc: 0.9603 - val_loss: 0.0808 - val_acc: 0.9000\n",
      "Epoch 262/300\n",
      "906/906 [==============================] - 0s 43us/step - loss: 0.0429 - acc: 0.9525 - val_loss: 0.0708 - val_acc: 0.9111\n",
      "Epoch 263/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0396 - acc: 0.9592 - val_loss: 0.1141 - val_acc: 0.8667\n",
      "Epoch 264/300\n",
      "906/906 [==============================] - 0s 54us/step - loss: 0.0425 - acc: 0.9558 - val_loss: 0.0872 - val_acc: 0.8778\n",
      "Epoch 265/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0378 - acc: 0.9636 - val_loss: 0.0743 - val_acc: 0.9000\n",
      "Epoch 266/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0355 - acc: 0.9658 - val_loss: 0.1016 - val_acc: 0.8556\n",
      "Epoch 267/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0437 - acc: 0.9503 - val_loss: 0.0782 - val_acc: 0.8889\n",
      "Epoch 268/300\n",
      "906/906 [==============================] - 0s 51us/step - loss: 0.0406 - acc: 0.9592 - val_loss: 0.0903 - val_acc: 0.8889\n",
      "Epoch 269/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0381 - acc: 0.9625 - val_loss: 0.0709 - val_acc: 0.9000\n",
      "Epoch 270/300\n",
      "906/906 [==============================] - 0s 49us/step - loss: 0.0359 - acc: 0.9658 - val_loss: 0.0843 - val_acc: 0.8889\n",
      "Epoch 271/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0390 - acc: 0.9625 - val_loss: 0.0718 - val_acc: 0.9222\n",
      "Epoch 272/300\n",
      "906/906 [==============================] - 0s 52us/step - loss: 0.0383 - acc: 0.9603 - val_loss: 0.1033 - val_acc: 0.8778\n",
      "Epoch 273/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0455 - acc: 0.9492 - val_loss: 0.0862 - val_acc: 0.8889\n",
      "Epoch 274/300\n",
      "906/906 [==============================] - 0s 42us/step - loss: 0.0358 - acc: 0.9658 - val_loss: 0.0730 - val_acc: 0.9111\n",
      "Epoch 275/300\n",
      "906/906 [==============================] - 0s 48us/step - loss: 0.0390 - acc: 0.9603 - val_loss: 0.0942 - val_acc: 0.8778\n",
      "Epoch 276/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0428 - acc: 0.9558 - val_loss: 0.0851 - val_acc: 0.8889\n",
      "Epoch 277/300\n",
      "906/906 [==============================] - 0s 48us/step - loss: 0.0362 - acc: 0.9647 - val_loss: 0.0805 - val_acc: 0.9111\n",
      "Epoch 278/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0419 - acc: 0.9558 - val_loss: 0.0804 - val_acc: 0.8778\n",
      "Epoch 279/300\n",
      "906/906 [==============================] - 0s 46us/step - loss: 0.0379 - acc: 0.9625 - val_loss: 0.0806 - val_acc: 0.9000\n",
      "Epoch 280/300\n",
      "906/906 [==============================] - 0s 50us/step - loss: 0.0388 - acc: 0.9592 - val_loss: 0.0816 - val_acc: 0.8889\n",
      "Epoch 281/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0407 - acc: 0.9570 - val_loss: 0.0726 - val_acc: 0.9111\n",
      "Epoch 282/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0370 - acc: 0.9636 - val_loss: 0.0778 - val_acc: 0.9000\n",
      "Epoch 283/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0434 - acc: 0.9536 - val_loss: 0.0726 - val_acc: 0.9000\n",
      "Epoch 284/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0357 - acc: 0.9658 - val_loss: 0.0755 - val_acc: 0.8889\n",
      "Epoch 285/300\n",
      "906/906 [==============================] - 0s 57us/step - loss: 0.0400 - acc: 0.9581 - val_loss: 0.0785 - val_acc: 0.9111\n",
      "Epoch 286/300\n",
      "906/906 [==============================] - 0s 51us/step - loss: 0.0391 - acc: 0.9603 - val_loss: 0.0923 - val_acc: 0.9000\n",
      "Epoch 287/300\n",
      "906/906 [==============================] - 0s 58us/step - loss: 0.0365 - acc: 0.9636 - val_loss: 0.0795 - val_acc: 0.8889\n",
      "Epoch 288/300\n",
      "906/906 [==============================] - 0s 51us/step - loss: 0.0357 - acc: 0.9647 - val_loss: 0.0813 - val_acc: 0.9000\n",
      "Epoch 289/300\n",
      "906/906 [==============================] - 0s 52us/step - loss: 0.0445 - acc: 0.9503 - val_loss: 0.0810 - val_acc: 0.9000\n",
      "Epoch 290/300\n",
      "906/906 [==============================] - 0s 48us/step - loss: 0.0356 - acc: 0.9647 - val_loss: 0.1070 - val_acc: 0.8778\n",
      "Epoch 291/300\n",
      "906/906 [==============================] - 0s 53us/step - loss: 0.0423 - acc: 0.9547 - val_loss: 0.0822 - val_acc: 0.9000\n",
      "Epoch 292/300\n",
      "906/906 [==============================] - 0s 51us/step - loss: 0.0415 - acc: 0.9570 - val_loss: 0.0820 - val_acc: 0.8778\n",
      "Epoch 293/300\n",
      "906/906 [==============================] - 0s 48us/step - loss: 0.0360 - acc: 0.9625 - val_loss: 0.0764 - val_acc: 0.9000\n",
      "Epoch 294/300\n",
      "906/906 [==============================] - 0s 53us/step - loss: 0.0405 - acc: 0.9592 - val_loss: 0.0711 - val_acc: 0.9111\n",
      "Epoch 295/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0343 - acc: 0.9680 - val_loss: 0.0746 - val_acc: 0.9000\n",
      "Epoch 296/300\n",
      "906/906 [==============================] - 0s 48us/step - loss: 0.0404 - acc: 0.9592 - val_loss: 0.0739 - val_acc: 0.9111\n",
      "Epoch 297/300\n",
      "906/906 [==============================] - 0s 45us/step - loss: 0.0364 - acc: 0.9647 - val_loss: 0.1009 - val_acc: 0.8778\n",
      "Epoch 298/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0454 - acc: 0.9448 - val_loss: 0.0776 - val_acc: 0.9000\n",
      "Epoch 299/300\n",
      "906/906 [==============================] - 0s 47us/step - loss: 0.0368 - acc: 0.9625 - val_loss: 0.0764 - val_acc: 0.9111\n",
      "Epoch 300/300\n",
      "906/906 [==============================] - 0s 49us/step - loss: 0.0360 - acc: 0.9647 - val_loss: 0.0726 - val_acc: 0.9000\n"
     ]
    }
   ],
   "source": [
    "#The simplest model is defined in the Sequential class which is a linear stack of Layers.\n",
    "X=data.iloc[:,0:18]\n",
    "y=data.iloc[:,19]\n",
    "seed=2\n",
    "np.random.seed(seed)\n",
    "(X_train,X_test,y_train,y_test)=train_test_split(X,y,test_size=0.09,random_state=seed)\n",
    "#Creating a model sequentially\n",
    "model=Sequential() \n",
    "model.add(Dense(32,init='uniform',input_dim=18,activation='relu'))#every neuron is densely connected to the neuron of next layer\n",
    "model.add(Dense(64,init='uniform',activation='relu')) #init: uniform normal distribution of weights()\n",
    "model.add(Dense(1,init='uniform',activation='sigmoid'))#output layer\n",
    "\n",
    "#Weights are initialized to small uniformly random values between 0 and 0.05 using init\n",
    "\n",
    "model.compile(loss='mean_squared_error',optimizer='rmsprop',metrics=['accuracy'])\n",
    "# loss:used to evaluate a set of weights. It is needed to reduce the error between actual output and expected output\n",
    "# optimizer: gradient descent to optimize weights\n",
    "\n",
    "a=model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=300,batch_size=100)\n",
    "#epochs: no. of training cycles\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8138679786970653\n"
     ]
    }
   ],
   "source": [
    "#for training\n",
    "y_pred = model.predict_classes(X_train)\n",
    "y_true=y_train;\n",
    "R_square=r2_score(y_true, y_pred, sample_weight=None, multioutput='uniform_average');\n",
    "print(R_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5852534562211982\n"
     ]
    }
   ],
   "source": [
    "#for validation \n",
    "y_pred = model.predict_classes(X_test)\n",
    "y_true=y_test;\n",
    "R_square=r2_score(y_true, y_pred, sample_weight=None, multioutput='uniform_average');\n",
    "print(R_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9421965317919074\n"
     ]
    }
   ],
   "source": [
    "#training f1 score\n",
    "y_pred = model.predict_classes(X)\n",
    "print(f1_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8148148148148148\n"
     ]
    }
   ],
   "source": [
    "#testing f1 score\n",
    "y_pred = model.predict_classes(X_test)\n",
    "print(f1_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
